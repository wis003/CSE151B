{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./Data/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class Pred(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbc1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Pred()\n",
    "opt = optim.Adam(pred.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa6608",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4725237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "epoch 0 loss: 1634624.9248961676\n",
      "epoch 1 loss: 509429.6373264348\n",
      "epoch 2 loss: 372467.9686743659\n",
      "epoch 3 loss: 309067.2268192622\n",
      "epoch 4 loss: 284505.80188814073\n",
      "epoch 5 loss: 246720.6271205819\n",
      "epoch 6 loss: 215982.2516756155\n",
      "epoch 7 loss: 196358.7343233841\n",
      "epoch 8 loss: 176074.1210879532\n",
      "epoch 9 loss: 164735.27213289405\n",
      "miami\n",
      "epoch 0 loss: 747228.862616736\n",
      "epoch 1 loss: 595068.3758197145\n",
      "epoch 2 loss: 548281.5615821584\n",
      "epoch 3 loss: 419461.60409196414\n",
      "epoch 4 loss: 406702.9160870471\n",
      "epoch 5 loss: 504897.36161355523\n",
      "epoch 6 loss: 352801.7256975416\n",
      "epoch 7 loss: 340679.6941178063\n",
      "epoch 8 loss: 339171.13813718245\n",
      "epoch 9 loss: 379063.1786120027\n",
      "pittsburgh\n",
      "epoch 0 loss: 113867.86084644332\n",
      "epoch 1 loss: 94283.5504309847\n",
      "epoch 2 loss: 84175.75139716851\n",
      "epoch 3 loss: 78688.81342648862\n",
      "epoch 4 loss: 76707.91519718764\n",
      "epoch 5 loss: 75735.75101396455\n",
      "epoch 6 loss: 75769.27801170785\n",
      "epoch 7 loss: 81864.08904278431\n",
      "epoch 8 loss: 76945.29813044026\n",
      "epoch 9 loss: 73318.09725041011\n",
      "dearborn\n",
      "epoch 0 loss: 260289.70542030726\n",
      "epoch 1 loss: 184290.4077976555\n",
      "epoch 2 loss: 186366.33759624138\n",
      "epoch 3 loss: 186395.06558383917\n",
      "epoch 4 loss: 181041.7669082691\n",
      "epoch 5 loss: 174455.7757011226\n",
      "epoch 6 loss: 176707.3302176539\n",
      "epoch 7 loss: 184988.08091557285\n",
      "epoch 8 loss: 171409.15741089126\n",
      "epoch 9 loss: 168994.42361909265\n",
      "washington-dc\n",
      "epoch 0 loss: 89093.73154025999\n",
      "epoch 1 loss: 72219.49420164934\n",
      "epoch 2 loss: 75686.0421977694\n",
      "epoch 3 loss: 72355.6189085779\n",
      "epoch 4 loss: 66902.69232862498\n",
      "epoch 5 loss: 71551.21831271618\n",
      "epoch 6 loss: 62968.94480240771\n",
      "epoch 7 loss: 66241.77696713619\n",
      "epoch 8 loss: 65826.61284816638\n",
      "epoch 9 loss: 61389.3007246172\n",
      "palo-alto\n",
      "epoch 0 loss: 475931.45703755\n",
      "epoch 1 loss: 141497.10480772916\n",
      "epoch 2 loss: 138472.1861570264\n",
      "epoch 3 loss: 116802.24687484908\n",
      "epoch 4 loss: 101730.34502498711\n",
      "epoch 5 loss: 99339.55700906474\n",
      "epoch 6 loss: 89374.54858721283\n",
      "epoch 7 loss: 94673.52469725849\n",
      "epoch 8 loss: 87379.05904743602\n",
      "epoch 9 loss: 85585.02703631532\n"
     ]
    }
   ],
   "source": [
    "batch_sz = 4  # batch size \n",
    "for city in cities:\n",
    "    print(city)\n",
    "    train_dataset = ArgoverseDataset(city = city, split = 'train')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_sz)\n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        total_loss = 0\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            inp, out = sample_batch\n",
    "            preds = pred(inp)\n",
    "            loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print('epoch {} loss: {}'.format(epoch, total_loss / len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f0079",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da57dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 209867.5162729818\n"
     ]
    }
   ],
   "source": [
    "val_dataset = ArgoverseDataset(city = 'austin', split = 'val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_sz)\n",
    "\n",
    "val_loss = 0\n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    preds = pred(inp)\n",
    "    loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "    val_loss += loss.item()\n",
    "print('loss: {}'.format(val_loss / len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9003de8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4feb8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "miami\n",
      "pittsburgh\n",
      "dearborn\n",
      "washington-dc\n",
      "palo-alto\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(ROOT_PATH + 'submission.csv')\n",
    "int_col = df.select_dtypes(include=['int'])\n",
    "for col in int_col.columns.values:\n",
    "    df[col] = df[col].astype('float32')\n",
    "row = 0\n",
    "\n",
    "for city in cities:\n",
    "    print(city)\n",
    "    test_dataset = ArgoverseDataset(city = city, split = 'test')\n",
    "\n",
    "    for i in range(len(test_dataset.inputs)):\n",
    "        preds = pred(torch.from_numpy(test_dataset.inputs[i]))\n",
    "        df.iloc[row, 1:121] = preds.detach().numpy().ravel()\n",
    "        row += 1\n",
    "        \n",
    "df.to_csv(ROOT_PATH + 'submission.csv')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02b62734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0      0\n",
      "v1      0\n",
      "v2      0\n",
      "v3      0\n",
      "v4      0\n",
      "       ..\n",
      "v115    0\n",
      "v116    0\n",
      "v117    0\n",
      "v118    0\n",
      "v119    0\n",
      "Name: 0, Length: 120, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0, 1:121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f31e4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-13.622161  405.25217   -13.65951   405.29318   -13.73504   405.29575\n",
      " -13.8152275 405.3567    -13.901553  405.45712   -14.004041  405.52954\n",
      " -14.096269  405.60107   -14.116809  405.67938   -14.175305  405.77536\n",
      " -14.335941  405.90155   -14.325156  405.95773   -14.337722  405.95886\n",
      " -14.290741  405.98138   -14.521394  406.05032   -14.562526  406.08014\n",
      " -14.500876  406.12805   -14.506674  406.18036   -14.482896  406.2714\n",
      " -14.52232   406.27576   -14.611872  406.40244   -14.595396  406.41562\n",
      " -14.749883  406.4368    -14.769161  406.58658   -14.776707  406.57822\n",
      " -14.821599  406.62808   -14.867535  406.65683   -14.8936405 406.725\n",
      " -15.023832  406.79147   -14.873926  406.80676   -14.986387  406.95242\n",
      " -14.921481  406.97784   -15.079258  407.01758   -15.026589  407.03906\n",
      " -15.174707  407.0621    -15.126724  407.09937   -15.221514  407.14175\n",
      " -15.187416  407.13995   -15.3077345 407.336     -15.255352  407.24374\n",
      " -15.253522  407.3635    -15.387627  407.4743    -15.38082   407.32138\n",
      " -15.457404  407.47766   -15.380844  407.50958   -15.533031  407.67432\n",
      " -15.549265  407.73184   -15.654754  407.73474   -15.693554  407.77835\n",
      " -15.731141  407.92413   -15.903545  407.81735   -15.82589   407.79324\n",
      " -15.774017  408.00186   -15.876906  408.02118   -16.112194  407.99915\n",
      " -16.05925   408.19702   -16.085892  408.2407    -16.2524    408.21756\n",
      " -16.122107  408.30423   -16.189524  408.28006   -16.230722  408.33694  ]\n",
      "-13.622161\n",
      "405.25217\n",
      "-13.65951\n"
     ]
    }
   ],
   "source": [
    "print(preds.detach().numpy().ravel())\n",
    "print(preds.detach().numpy().ravel()[0])\n",
    "print(preds.detach().numpy().ravel()[1])\n",
    "print(preds.detach().numpy().ravel()[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    print(inp.shape, out.shape)\n",
    "    break\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
