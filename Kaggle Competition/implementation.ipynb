{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./Data/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "\n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "#         n = len(inputs)\n",
    "#         inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "#         outputs = pickle.load(open(f_out, \"rb\"))\n",
    "#         outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Pred(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 120)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 100).float()\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.reshape(-1, 60, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbc1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa6608",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4725237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "epoch 0 loss: 3077572.0323735126\n",
      "epoch 1 loss: 697965.4520740185\n",
      "epoch 2 loss: 532024.6863985641\n",
      "epoch 3 loss: 436032.7385336218\n",
      "epoch 4 loss: 369642.1275699391\n",
      "epoch 5 loss: 360097.7329328761\n",
      "epoch 6 loss: 311659.6386696401\n",
      "epoch 7 loss: 270186.44504187134\n",
      "epoch 8 loss: 252034.79141662922\n",
      "epoch 9 loss: 220792.85495155051\n",
      "epoch 10 loss: 207633.8321390795\n",
      "epoch 11 loss: 200407.9707164495\n",
      "epoch 12 loss: 182725.17695131406\n",
      "epoch 13 loss: 167862.14971844185\n",
      "epoch 14 loss: 164970.3672180274\n",
      "epoch 15 loss: 159950.73468992652\n",
      "epoch 16 loss: 148265.84365515027\n",
      "epoch 17 loss: 141055.15880360443\n",
      "epoch 18 loss: 142186.2307339766\n",
      "epoch 19 loss: 141381.0413930823\n",
      "miami\n",
      "epoch 0 loss: 998882.9426329614\n",
      "epoch 1 loss: 598813.4630062662\n",
      "epoch 2 loss: 469329.74156252126\n",
      "epoch 3 loss: 533441.6913732116\n",
      "epoch 4 loss: 447417.13040979515\n",
      "epoch 5 loss: 460983.1030635889\n",
      "epoch 6 loss: 429939.6294615634\n",
      "epoch 7 loss: 446552.14377611206\n",
      "epoch 8 loss: 413607.4266737351\n",
      "epoch 9 loss: 401890.4841889047\n",
      "epoch 10 loss: 495753.146871254\n",
      "epoch 11 loss: 366708.3857465168\n",
      "epoch 12 loss: 429508.13402403996\n",
      "epoch 13 loss: 357133.68269267416\n",
      "epoch 14 loss: 338129.3624085459\n",
      "epoch 15 loss: 314836.63793390064\n",
      "epoch 16 loss: 367117.3645429357\n",
      "epoch 17 loss: 344221.8753229616\n",
      "epoch 18 loss: 329906.2279889463\n",
      "epoch 19 loss: 313233.51348294964\n",
      "pittsburgh\n",
      "epoch 0 loss: 138637.08275032675\n",
      "epoch 1 loss: 96316.03823448658\n",
      "epoch 2 loss: 87422.48226840554\n",
      "epoch 3 loss: 85618.2219192202\n",
      "epoch 4 loss: 80501.48077131149\n",
      "epoch 5 loss: 74037.08500666088\n",
      "epoch 6 loss: 70594.19576459036\n",
      "epoch 7 loss: 69422.2341109765\n",
      "epoch 8 loss: 67111.52617952989\n",
      "epoch 9 loss: 65284.68486178833\n",
      "epoch 10 loss: 61665.25808675028\n",
      "epoch 11 loss: 64831.264252526584\n",
      "epoch 12 loss: 60801.18535048076\n",
      "epoch 13 loss: 60033.374689072516\n",
      "epoch 14 loss: 58940.46074855861\n",
      "epoch 15 loss: 58236.69465725117\n",
      "epoch 16 loss: 56977.52184438321\n",
      "epoch 17 loss: 55269.48265104608\n",
      "epoch 18 loss: 54870.27862371392\n",
      "epoch 19 loss: 55567.00891336204\n",
      "dearborn\n",
      "epoch 0 loss: 267296.60229567764\n",
      "epoch 1 loss: 153110.17023152584\n",
      "epoch 2 loss: 152074.42059175659\n",
      "epoch 3 loss: 150472.3297549375\n",
      "epoch 4 loss: 141323.85755397353\n",
      "epoch 5 loss: 143107.10748588882\n",
      "epoch 6 loss: 145963.28994038346\n",
      "epoch 7 loss: 146071.41159331807\n",
      "epoch 8 loss: 134702.70742294932\n",
      "epoch 9 loss: 139812.87266252292\n",
      "epoch 10 loss: 155516.56922189842\n",
      "epoch 11 loss: 143443.39913387227\n",
      "epoch 12 loss: 132458.58307947346\n",
      "epoch 13 loss: 129953.65962923672\n",
      "epoch 14 loss: 135210.2066773427\n",
      "epoch 15 loss: 140182.79387428475\n",
      "epoch 16 loss: 130594.90998460987\n",
      "epoch 17 loss: 138132.88327539532\n",
      "epoch 18 loss: 123279.70772403575\n",
      "epoch 19 loss: 126869.61790494285\n",
      "washington-dc\n",
      "epoch 0 loss: 70982.15733140007\n",
      "epoch 1 loss: 63998.354948182765\n",
      "epoch 2 loss: 65119.65039198433\n",
      "epoch 3 loss: 64280.63698727415\n",
      "epoch 4 loss: 58138.71012968943\n",
      "epoch 5 loss: 59425.708590639515\n",
      "epoch 6 loss: 58086.37913185451\n",
      "epoch 7 loss: 67031.16725607401\n",
      "epoch 8 loss: 59782.91599912991\n",
      "epoch 9 loss: 59927.652527756785\n",
      "epoch 10 loss: 58426.71259771103\n",
      "epoch 11 loss: 57241.608377740464\n",
      "epoch 12 loss: 60190.326171699184\n",
      "epoch 13 loss: 58301.19929103353\n",
      "epoch 14 loss: 56763.835869900075\n",
      "epoch 15 loss: 55697.355815441966\n",
      "epoch 16 loss: 56341.581419444796\n",
      "epoch 17 loss: 54207.93404851061\n",
      "epoch 18 loss: 54242.55370799986\n",
      "epoch 19 loss: 52477.55134030571\n",
      "palo-alto\n",
      "epoch 0 loss: 400830.2084050922\n",
      "epoch 1 loss: 173049.09981709463\n",
      "epoch 2 loss: 125964.75422354987\n",
      "epoch 3 loss: 104773.17064260505\n",
      "epoch 4 loss: 101678.5186043543\n",
      "epoch 5 loss: 96737.24000809857\n",
      "epoch 6 loss: 101807.12680338444\n",
      "epoch 7 loss: 95002.2101745282\n",
      "epoch 8 loss: 92999.180605202\n",
      "epoch 9 loss: 90404.34097037006\n",
      "epoch 10 loss: 92730.94895514045\n",
      "epoch 11 loss: 91241.92033429413\n",
      "epoch 12 loss: 82089.97727553298\n",
      "epoch 13 loss: 87471.93932604119\n",
      "epoch 14 loss: 93106.116664179\n",
      "epoch 15 loss: 83833.94020627518\n",
      "epoch 16 loss: 84557.9628362159\n",
      "epoch 17 loss: 81894.09353197414\n",
      "epoch 18 loss: 83456.87509345682\n",
      "epoch 19 loss: 85812.13368604041\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(ROOT_PATH + 'submission.csv')\n",
    "int_col = df.select_dtypes(include=['int'])\n",
    "for col in int_col.columns.values:\n",
    "    df[col] = df[col].astype('float32')\n",
    "row = 0\n",
    "\n",
    "batch_sz = 4  # batch size\n",
    "\n",
    "for city in cities:\n",
    "    print(city)\n",
    "    \n",
    "    pred = Pred()\n",
    "    pred.to(device)\n",
    "    opt = optim.Adam(pred.parameters(), lr=1e-3)\n",
    "    train_dataset = ArgoverseDataset(city = city, split = 'train')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_sz)\n",
    "\n",
    "    for epoch in range(20):\n",
    "\n",
    "        total_loss = 0\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            i, o = sample_batch\n",
    "            inp, out = i.to(device), o.to(device)\n",
    "            preds = pred(inp)\n",
    "            loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print('epoch {} loss: {}'.format(epoch, total_loss / len(train_dataset)))\n",
    "    \n",
    "    print(\"TESTING \" + city)\n",
    "    test_dataset = ArgoverseDataset(city = city, split = 'test')\n",
    "\n",
    "    for i in range(len(test_dataset.inputs)):\n",
    "        data = torch.from_numpy(test_dataset.inputs[i]).to(device)\n",
    "        preds = pred(data)\n",
    "        df.iloc[row, 1:121] = preds.cpu().detach().numpy().ravel()\n",
    "        row += 1\n",
    "        \n",
    "df.to_csv(ROOT_PATH + 'submission.csv')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f0079",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da57dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 260554.3922533004\n"
     ]
    }
   ],
   "source": [
    "# val_dataset = ArgoverseDataset(city = 'austin', split = 'val')\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_sz)\n",
    "\n",
    "# val_loss = 0\n",
    "# for i_batch, sample_batch in enumerate(val_loader):\n",
    "#     i, o = sample_batch\n",
    "#     inp, out = i.to(device), o.to(device)\n",
    "#     preds = pred(inp)\n",
    "#     loss = ((preds - out) ** 2).sum()\n",
    "\n",
    "#     val_loss += loss.item()\n",
    "# print('loss: {}'.format(val_loss / len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04bcdc",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09cbc943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austin\n",
      "miami\n",
      "pittsburgh\n",
      "dearborn\n",
      "washington-dc\n",
      "palo-alto\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "\n",
    "\n",
    "# def show_sample_batch(sample_batch):\n",
    "#     \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "#     inp, out = sample_batch\n",
    "#     batch_sz = inp.size(0)\n",
    "#     agent_sz = inp.size(1)\n",
    "    \n",
    "#     fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "#     axs = axs.ravel()   \n",
    "#     for i in range(batch_sz):\n",
    "#         axs[i].xaxis.set_ticks([])\n",
    "#         axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "#         # first two feature dimensions are (x,y) positions\n",
    "#         axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "#         axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "# for i_batch, sample_batch in enumerate(train_loader):\n",
    "#     inp, out = sample_batch\n",
    "#     print(inp.shape, out.shape)\n",
    "#     break\n",
    "#     \"\"\"\n",
    "#     TODO:\n",
    "#       implement your Deep learning model\n",
    "#       implement training routine\n",
    "#     \"\"\"\n",
    "#     show_sample_batch(sample_batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
